---
title: "Bayesian Analysis of Vertical Configuration, For Preregistration"
author: "Alex Holcombe"
date: "8/06/2019"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For an experiment in which in one condition, two letters are presented in vertical configuration, and in other condition, two words are presented in vertical configuration. (half of trials in left visual field and half in right visual field, but will probably collapse across the two visual fields for most of these analyses)

The prereg of the experiment with the incorrect vertical alignment is [here](https://osf.io/jpy58).

Illustrate the Bayesian analysis to be done.

To inform the prior for the size of the letters top advantage, we will use the previously-published vertical results.

But what about the fact that in the erroneous (vertical position wrong) experiment, the reading bias was lower than before. Well, that was the horizontal one. And it could be a fluke, or it could be again because of left-hemisphere specialization or the verbal response. Just additional reasons to be conservative.

Another difference with our previous work is that in this experiment both streams are in one or the other hemisphere.

Average the results from Holcombe Nguyen & Goodbourn (backwards paper), E2 because that had the vertical configuration, with Holcombe & Goodbourn () vertical. However, the letters in the Holcombe Nguyen & Goodbourn experiment were rotated to face downwards or upwards so weight the Goodbourn & Holcombe effect more highly.


```{r, echo=FALSE}
#E1, E2 refers to Holcombe Nguyen Goodbourn E1, E2
E1n<- 16
#Raw effect size plus standard error and sd
P1E1canonical=.23;  P1E1c_se = .058; P1E1c_sd = P1E1c_se * sqrt(E1n)
P1E1reversed= .062;  P1E1r_se = .07; P1E1r_sd = P1E1r_se * sqrt(E1n)
#To calculate sd of the bias diff, consider that the variances sum,
#therefore first sum the variances then convert back to standard deviation
#Although only true for *independent* random variables and these unlikely to be totally independent

E2n<- 24
P1E2canonical=.218; P1E2c_se =.034; P1E2c_sd= P1E2c_se * sqrt(E2n)
P1E2inverted=.017; P1E2i_se =.049; P1E2i_sd = P1E2i_se * sqrt(E2n)

P1E2vertUpright=.188; P1E2vu_se=.043; P1E2vu_sd = P1E2vu_se * sqrt(E2n)
P1E2vertInverted= -.07; P1E2vi_se=.049; P1E2vi_sd = P1E2vi_se * sqrt(E2n)

```

What was it for the HolcombeNguyenGoodbourn paper? For E1 horizontal,`r P1E1canonical`canonical orientation and 
`r P1E1reversed` Mirror-reversed. For E2, `r P1E2canonical` for canonical orientation.

For E2 vertical arrangement, letters rotated to face down, `r P1E2vertUpright` and for letters facing up, `r P1E2vertInverted`.


```{r HolcombeGoodbourn, echo = FALSE}
#Figure 4a
#Looks like for HolcombeNguyenGoodbourn the effect size calculations above are all of the difference scores, not the individual sides
# vertical difference d = 1.74, 95% CI [0.30, 0.50].  This is taken straight from the results section.
HolGoodVertCohensD = 1.74

```

For Holcombe & Goodbourn, vertical upper advantage was Cohen's d of `r HolGoodVertCohensD`.

Do Bayesian t-test. That is, take the likelihood of the left bias difference observed under the prior and divide it by the likelihood of the left bias difference observed under the null.

http://jeffrouder.blogspot.com.au/2016/01/what-priors-should-i-use-part-i.html
For the null model, the bias is just zero (delta function).For our model, support is zero to infinity. 

Probably better to do it in standardized units (Cohen's d) because otherwise need to create a model of the variance separately (for the data model) so that can calculate the probability of the data given e.g. the null hypothesis.

```{r}
cohensDsHoriz<-c(P1E1canonical/P1E1c_sd,
                 P1E2canonical/P1E2c_sd) 
cohensDsHorizLabels<-c("HolNguGoodE1horiz","HolNguGoodE2horiz")

cohensDs<-c(HolGoodVertCohensD,
            P1E2vertUpright / P1E2vu_sd) 
print(cohensDs)
cohensDslabels<-c("HolGoodVert", "HolNguGoodVert")
meanCohensD<-cohensDs[[1]]*.7 + cohensDs[[2]]*.3  #mean(cohensDs)

modalEffectLtrs = meanCohensD
```
We are predicting (the mode of our prior) for the letters condition is `r modalEffectLtrs`.

The words arguably have a greater left-to-right implication than individual letters, suggesting the words effect will be smaller than the letters effect. On the other hand, the words could kick reading processes more than individual letters, suggesting it could be an even greater effect. On the third hand, the letters bias is already so much bigger than conventional effects in psychology that it's not likely that it would be bigger, this third hand weighs the balance slightly toward smaller effects, so we can expect that the effect will be smaller than the usual letters effect,
```{r, echo=FALSE}
wordHandicap=.84
```
by a factor of `r wordHandicap`. Also it's much less certain than the letters so should make the sigmas bigger, which we'll do later.

```{r handicapWords}
wordPredictedCohensD<- meanCohensD * wordHandicap
```

As a result we are predicting (the mode of our prior) that the Cohen's d for the words condition upper bias is `r wordPredictedCohensD`.

For the shape of the prior, I decided on what I call a "double Gaussian" with different sd to left of mode and right of mode (later it will be rescaled to make it a density).

For the width of the prior, to be honest it is hard to know how to set a Gaussian sigma (width of the prior) because how much to think of the previous estimates as estimating the same thing versus being actually different effects? Setting it to the sd of the previous results looks too small. By trial and error with eyeball I arrived at sigma on each side of
```{r}
sigmaLeft =.93
sigmaRight = .5
```

This is based on wanting right of the mode of the distribution to be thin-tailed (because the true effect is almost definitely not enormous, like greater than 2.5) while the left side to be heavier (because the effect might well end up being hella closer to zero), I will use two half-Gaussians with different standard deviations. 

```{r, echo=FALSE}
#Create doubleGaussian ,and vectorize it because common functions like plot require it , otherwise you have to use mapply

doubleGaussian = function(xs, mean,sdLeft,sdRight) {
  #print(length(x))  # Most plotting functions send this a million numbers rather than one
  #A double gaussian is two half-gaussians with different sigmas. But to make them match up, I have to calculate the peak height of each and then rescale. The peak height is of course its value at the mean. I will scale each so that it peaks at 1.
  #It will then be rescaled later to make it a proper density function that integrates to 1.
  leftXs <- xs[xs<=mean]
  rightXs <- xs[xs>mean]
  Ys<-c()
  if (length(leftXs) >0) {
    leftPeakHeight = dnorm(mean,mean,sdLeft)
    Ys <- dnorm(leftXs,mean,sdLeft) / leftPeakHeight
  }
  if (length(rightXs) >0) {
    rightPeakHeight = dnorm(mean,mean,sdRight)
    rightYs <- dnorm(rightXs,mean,sdRight) / rightPeakHeight
    Ys <- c(Ys,rightYs)
  }
  Ys
}

myDoubleGaussian = function(x) {
  doubleGaussian(x,0,2,1)
}

#Test that myDoubleGaussian is working.

myDoubleGaussian(3)
```
Plot doubleGaussian graphs to make sure it's working.
```{r, echo=FALSE}

#My doubleGaussian function is not vectorized so I can't use normal way of plotting. Maybe I should vectorize, but that would be a good deal of work.
#https://stackoverflow.com/questions/44730774/how-to-use-custom-functions-in-mutate-dplyr

library("dplyr")
df <- data.frame(x=seq(-1,1,.01))
#df<- df %>% mutate(density=  mapply(function(x) myDoubleGaussian(x), x))
df<- df %>% mutate(density=myDoubleGaussian(x))

library('ggplot2')
ggplot(df, aes(x=x,y=density)) + geom_line() + xlab("x") +theme_bw()
```


```{r }

#Set up my prior, a double Gaussian, making it a function of just one parameter, the x value
doubleGaussianLtrs = function(x) {
  doubleGaussian(x,modalEffectLtrs,sigmaLeft,sigmaRight)
}
```

Create integrable function for the prior
```{r lettersPrior}
#Specify hypothesis, technically alternative hypothesis (up to constant of proportionality) for letters
lo=0 #lower bound of support
hi=Inf #upper bound of support

#Define integrable prior - must be function of only the integrand
altHypothesisLtrs=function(delta) {
  y= doubleGaussianLtrs(delta)
  y=y*as.integer(delta>lo)*as.integer(delta<hi) #multiply by zero if outside of support
}

```

```{r }
#Normalize alternative density in case is not already
#Calculate normalization factor - fraction to multiply prior by so it will integrate to 1
K=1/integrate(altHypothesisLtrs,lower=lo,upper=hi)$value 

#Thus final prior is the normalized function
priorLtrs=function(delta) K*altHypothesisLtrs(delta)
```

Plot the letters prior, a DoubleGaussian distribution with our mean observed effect (as a Cohen's d), `r modalEffectLtrs` and sigma left of mode=`r sigmaLeft` and sigma right of mode=`r sigmaRight`.

```{r}
dDomain=seq(0,3,.01) #density domain

#Plot Alternative as a density and Null as a point arrow
maxAlt = priorLtrs(modalEffectLtrs) #maxAlt=max(priorLtrs(dDomain))

library("dplyr")
df <- data.frame(x=dDomain)
df<- df %>% mutate(density=  mapply(function(x) myDoubleGaussian(x), x))
#df<- df %>% mutate(density=x) #mutate(density=myDoubleGaussian(x))

library('ggplot2')
g<-ggplot(df, aes(x=x,y=density)) + geom_line() + xlab("Reading-direction bias (Cohen's d)") + theme_bw() 
g<- g + ggtitle("Letter top bias prior")
show(g)

plot(dDomain,priorLtrs(dDomain),typ='n',xlab="Reading-direction bias (Cohen's d)",ylab="Density",ylim=c(0,max(1,1.4*maxAlt)),main=)
#typ 'n' means invisible, and it's the lines command that actually plots the points.
arrows(0,0,0,1,col='darkblue',lwd=2)
lines(dDomain,priorLtrs(dDomain),col='green',lwd=2)
legend("topright",legend=c("Null","Alternative","Previous findings"),col=c('darkblue','green','black'),lwd=2)
points(cohensDs, rep(.1,length(cohensDs)),pch=19) #plot previous vertical effect sizes
text(cohensDs, c(.2,.27), cohensDslabels,cex=.75)

points(cohensDsHoriz, rep(.1,length(cohensDsHoriz)),pch=0) #plot previous horizontal arrangement effect sizes
text(cohensDsHoriz, c(.02,.06), cohensDsHorizLabels,cex=.75, col="grey")

```

```{r}
sigmaLeftWords = sigmaLeft
sigmaRightWords = 0.9
```
In contrast to the letters, prior for the words will be  a DoubleGaussian distribution with our mean observed effect (as a Cohen's d), `r wordPredictedCohensD` and sigma left of mode=`r sigmaLeft` and sigma right of mode=`r sigmaRight`.

```{r wordsPrior}
#Specify hypothesis, technically alternative hypothesis (up to constant of proportionality) for words

#Set up mhy prior, a double Gaussian, making it a function of just one parameter, the x value
myDoubleGaussian = function(x) {
  doubleGaussian(x,wordPredictedCohensD,sigmaLeftWords,sigmaRightWords)
}

#Specify hypothesis, technically alternative hypothesis (up to constant of proportionality) for letters
lo=0 #lower bound of support
hi=Inf #upper bound of support

#Define integrable prior - must be function of only the integrand
altHypothesis=function(delta) {
  y= myDoubleGaussian(delta)
  y=y*as.integer(delta>lo)*as.integer(delta<hi) #multiply by zero if outside of support
}

#Normalize alternative density in case is not already
#Calculate normalization factor - fraction to multiply prior by so it will integrate to 1
K=1/integrate(altHypothesis,lower=lo,upper=hi)$value 

#Thus final prior is the normalized function
priorWords=function(delta) K*altHypothesis(delta)
```

Plot the words prior

```{r, echo=FALSE}

#delta=seq(-.2,1.2,.01)

dDomain=seq(-.3,2.5,.01)

#Plot Alternative as a density and Null as a point arrow
maxAlt=max(priorWords(dDomain))
plot(dDomain,priorWords(dDomain),typ='n',xlab="Top bias (Cohen's d)",ylab="Density",ylim=c(0,1),main="Words and letters top bias prior")
arrows(0,0,0,1,col='darkblue',lwd=2)
lines(dDomain,priorWords(dDomain),col='yellow',lwd=3)
lines(dDomain,priorLtrs(dDomain),col='green',lwd=2)

legend("topright",legend=c("Null","AlternativeLtrs","AlternativeWords","Previous findings"),col=c('darkblue','green','yellow','black'),lwd=2)

points(cohensDs, rep(.1,length(cohensDs)),pch=19) #plot previous vertical effect sizes
text(cohensDs, c(.17,.17), cohensDslabels,cex=.75)

points(cohensDsHoriz, rep(.1,length(cohensDsHoriz)),pch=0) #plot previous horizontal arrangement effect sizes
text(cohensDsHoriz, c(.02,.06), cohensDsHorizLabels,cex=.75, col="grey")
```

When the data come in, will calculate the Cohen's d and then for each Cohen's d in the prior, calculate the probability of the data, integrating over the entire prior. That average likelihood (probability of the data under the alternative hypothesis) will be compared to probability of the data under the null hypothesis.

## Probability of any particular effect size according to these priors

If we take a sample of n observations from a normal distribution, then the t-distribution with n-1 degrees of freedom can be defined as the distribution of the location of the sample mean relative to the true mean, divided by the sample standard deviation, after multiplying by the standardizing term sqrt(n).

n refers to the number of participants, and the sample standard deviation is the standard deviation of the participant effects.

dt(x, df, ncp, log = FALSE) Density for the t distribution with df degrees of freedom (and optional non-centrality parameter ncp).


## Secondary research questions (not part of the main theory)

3. Whether the top bias depends on whether the streams are presented in the left or right hemifield. 
To the extent that there is a top bias, smaller capacity limit in one hemisphere, as suggested by so-and-so, could create a greater top bias.

The difference in efficacy between upper and lower streams for both letters and words depends on whether the streams are presented in the left or right visual field. STATISTICAL TEST: Compare top bias in LVF to top bias in RVF. Bayesian t-test should do it because I'm expecting a quite small effect size so the default psychology prior should be fine.

4. Previous research that has investigated lateral biases in word recognition has tended to find better performance for words presented to the right visual field, though studies that have measured this using nonwords or letter strings have found mixed results.  Overall performance may be better in the right visual field, especially for the words.

STATISTICAL TEST: ANOVA with Average efficacy  (of top and bottom) as d.v. and two factors: visual field and words versus letters.

