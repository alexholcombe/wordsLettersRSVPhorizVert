---
title: "Bayesian Analysis of Vertical Words and Letters experiment, For Preregistration"
author: "Alex Holcombe"
date: '`r paste("Mostly completed on Aug 15, 2019. Updated on", Sys.Date())`'

output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary

This is for an experiment by Kim Ransley and Alex Holcombe in which in one condition, two letters are presented in vertical configuration, and in other condition, two words are presented in vertical configuration (half of trials in left visual field and half in right visual field, but will probably collapse across the two visual fields for most of these analyses).

The prereg of the first attempt at the experiment with the incorrect (due to programming error) vertical alignment is [here](https://osf.io/jpy58).

To inform the prior for the size of the letters top advantage, we will use the previously-published vertical results.

### Hypothesis 1: The letters will show a top bias

### Hypothesis 2: The words will show a top bias

More specifically, these two hypotheses contrast the null hypothesis of no effect to the hypothesis of there being an effect in line with what was estimated in previous experiments.

## Dependent variables

Whereas in previous papers the primary dependent variable was efficacy, a parameter estimated by mixture modeling of temporal errors (Goodbourn & Holcombe, 2015), with the slow presentation rate needed for the words, temporal errors are not common enough to do mixture modeling. Instead, we use:

* "Efficacy" - efficacy (as defined by Goodbourn & Holcombe, 2015), but a rough measure modifed from Vul, Nieuwenstein, & Kanwisher (2008), of counting a response as correct if the target was reported or the item before or after the target in the stream
* "Loose efficacy" -Van Orden’s (1987) index quantified the orthographic similarity between the participant’s response and a correct word on each trial.  A complication was that a trial was counted as correct if participants reported an item that appeared temporally close to the target, and so it was first necessary to determine which item the participant’s response was based on.  To do this, the participant’s response was compared to all other words in the stream, and it was then assumed that the participant’s response was based on whatever word in the sequence was most orthographically similar.  We added the requirement that the orthographical similarity estimate of this word must be above 0.7 in order to avoid capturing trials where participants missed the target and guessed a word that was not in the stream.  

All of the Bayesian analyses described below were done with both both measures.

### Considerations for the prior on the effect size

In E1 (horizontally arrayed letters versus horizontally arrayed words), the left advantage of the letters was smaller than in all previous experiments, which is a bit worrying. It could be a fluke, or it could be because of half of Ss just done the words which givs a big right hemifield advantage, or it could be the oral response. All additional reasons to be conservative.

Another difference with our previous work is that in this experiment both streams are in one or the other hemisphere.

### Previous effect sizes

```{r, echo=FALSE}
#E1, E2 refers to Holcombe Nguyen Goodbourn E1, E2
E1n<- 16
#Raw effect size plus standard error and sd
P1E1canonical=.23;  P1E1c_se = .058; P1E1c_sd = P1E1c_se * sqrt(E1n)
P1E1reversed= .062;  P1E1r_se = .07; P1E1r_sd = P1E1r_se * sqrt(E1n)
#To calculate sd of the bias diff, consider that the variances sum,
#therefore first sum the variances then convert back to standard deviation
#Although only true for *independent* random variables and these unlikely to be totally independent

E2n<- 24
P1E2canonical=.218; P1E2c_se =.034; P1E2c_sd= P1E2c_se * sqrt(E2n)
P1E2inverted=.017; P1E2i_se =.049; P1E2i_sd = P1E2i_se * sqrt(E2n)

P1E2vertUpright=.188; P1E2vu_se=.043; P1E2vu_sd = P1E2vu_se * sqrt(E2n)
P1E2vertInverted= -.07; P1E2vi_se=.049; P1E2vi_sd = P1E2vi_se * sqrt(E2n)

cohensDsHoriz<-c(P1E1canonical/P1E1c_sd,
                 P1E2canonical/P1E2c_sd) 
cohensDsHorizLabels<-c("HolNguGoodE1horiz","HolNguGoodE2horiz")
```

For the HolcombeNguyenGoodbourn paper? For E1 (horizontal),`r P1E1canonical` in the canonical orientation and 
`r P1E1reversed` for mirror-reversed. For E2 (vertical) `r P1E2canonical` for canonical horizontal orientation.

For E2 vertical arrangement, letters rotated to face down, `r P1E2vertUpright` and for letters facing up, `r P1E2vertInverted`.


```{r HolcombeGoodbourn, echo = FALSE}
#Figure 4a
#Looks like for HolcombeNguyenGoodbourn the effect size calculations above are all of the difference scores, not the individual sides
# vertical difference d = 1.74, 95% CI [0.30, 0.50].  This is taken straight from the results section.
HolGoodVertCohensD = 1.74

```

For Holcombe & Goodbourn, vertical upper advantage was Cohen's d of `r HolGoodVertCohensD`.

### Calculating the mode of the prior for the effect size for letters and words

Average the results from Holcombe Nguyen & Goodbourn (backwards paper) E2 because that had the vertical configuration, with Goodbourn & Holcombe (2015) vertical configuration. The letters in the Holcombe Nguyen & Goodbourn experiment were rotated to face downwards or upwards, so weight the Goodbourn & Holcombe effect more highly, yielding a *predicted value for the letter upper bias of*
```{r, echo=FALSE}


cohensDs<-c(HolGoodVertCohensD,
            P1E2vertUpright / P1E2vu_sd) 
print(cohensDs)
cohensDslabels<-c("HolGoodVert", "HolNguGoodVert")
meanCohensD<-cohensDs[[1]]*.7 + cohensDs[[2]]*.3  #mean(cohensDs)

modalEffectLtrs = meanCohensD
```
`r modalEffectLtrs`

This value will be the mode of our prior, but of course there is plenty of uncertainty about what the true effect size might be even under this theory, so we will have a wide distribution.

The words arguably have a greater left-to-right implication than individual letters, suggesting the words effect will be smaller than the letters effect. On the other hand, the words could kick reading processes more than individual letters, suggesting it could be an even greater effect. On the third hand, the letters bias is already so much bigger than conventional effects in psychology that it's not likely that it would be bigger, this third hand weighs the balance slightly toward smaller effects, so we can expect that the effect will be smaller than the usual letters effect,
```{r, echo=FALSE}
wordHandicap=.84
```
by a factor of `r wordHandicap`. Also it's much less certain than the letters so should make the sigmas bigger, which we'll do later.

```{r handicapWords, echo=FALSE}
wordPredictedCohensD<- meanCohensD * wordHandicap
```

As a result we are predicting (the mode of our prior) that the Cohen's d for the words condition upper bias is `r wordPredictedCohensD`.

## Constructing the prior distribution and density (normalized distribution)

For the shape of the prior, we will use what I call a "double Gaussian" with different sd to the left of mode than to the right of the mode (later it will be rescaled to make it a density).

For the width of the prior, to be honest it is hard to know how to set a Gaussian sigma (width of the prior) because how much to think of the previous estimates as estimating the same thing versus being actually different effects? Setting it to the sd of the previous results looks too small. By trial and error with eyeball I arrived at sigma on each side of
```{r}
sigmaLeft =.93
sigmaRight = .5
```

This is based on wanting right of the mode of the distribution to be thin-tailed (because the true effect is almost definitely not enormous, like greater than 2.5) while the left side to be heavier (because the effect might well end up being hella closer to zero), I will use two half-Gaussians with different standard deviations. 

```{r, echo=FALSE}
#Create doubleGaussian ,and vectorize it because common functions like plot require it , otherwise you have to use mapply

doubleGaussian = function(xs, mean,sdLeft,sdRight) {
  #print(length(x))  # Most plotting functions send this a million numbers rather than one
  #A double gaussian is two half-gaussians with different sigmas. But to make them match up, I have to calculate the peak height of each and then rescale. The peak height is of course its value at the mean. I will scale each so that it peaks at 1.
  #It will then be rescaled later to make it a proper density function that integrates to 1.
  leftXs <- xs[xs<=mean]
  rightXs <- xs[xs>mean]
  Ys<-c()
  if (length(leftXs) >0) {
    leftPeakHeight = dnorm(mean,mean,sdLeft)
    Ys <- dnorm(leftXs,mean,sdLeft) / leftPeakHeight
  }
  if (length(rightXs) >0) {
    rightPeakHeight = dnorm(mean,mean,sdRight)
    rightYs <- dnorm(rightXs,mean,sdRight) / rightPeakHeight
    Ys <- c(Ys,rightYs)
  }
  Ys
}

myDoubleGaussian = function(x) {
  doubleGaussian(x,0,2,1)
}

#Test that myDoubleGaussian is working.

myDoubleGaussian(3)
```
Plot doubleGaussian graphs to make sure it's working.
```{r, echo=FALSE}

#My doubleGaussian function is not vectorized so I can't use normal way of plotting. Maybe I should vectorize, but that would be a good deal of work.
#https://stackoverflow.com/questions/44730774/how-to-use-custom-functions-in-mutate-dplyr

library("dplyr")
df <- data.frame(x=seq(-1,1,.01))
#df<- df %>% mutate(density=  mapply(function(x) myDoubleGaussian(x), x))
df<- df %>% mutate(density=myDoubleGaussian(x))

library('ggplot2')
ggplot(df, aes(x=x,y=density)) + geom_line() + xlab("x") +theme_bw()
```


```{r }

#Set up my prior, a double Gaussian, making it a function of just one parameter, the x value
doubleGaussianLtrs = function(x) {
  doubleGaussian(x,modalEffectLtrs,sigmaLeft,sigmaRight)
}
```

Create integrable function for the prior
```{r lettersPrior}
#Specify hypothesis, technically alternative hypothesis (up to constant of proportionality) for letters
lo=0 #lower bound of support
hi=Inf #upper bound of support

#Define integrable prior - must be function of only the integrand
altHypothesisLtrs=function(delta) {
  y= doubleGaussianLtrs(delta)
  y=y*as.integer(delta>lo)*as.integer(delta<hi) #multiply by zero if outside of support
}

```

```{r }
#Normalize alternative density in case is not already
#Calculate normalization factor - fraction to multiply prior by so it will integrate to 1
K=1/integrate(altHypothesisLtrs,lower=lo,upper=hi)$value 

#Thus final prior is the normalized function
priorLtrs=function(delta) K*altHypothesisLtrs(delta)
```

Plot the letters prior, a DoubleGaussian distribution with our mean observed effect (as a Cohen's d), `r modalEffectLtrs` and sigma left of mode=`r sigmaLeft` and sigma right of mode=`r sigmaRight`.

```{r, echo=FALSE}
dDomain=seq(0,3,.01) #density domain

#Plot Alternative as a density and Null as a point arrow
maxAlt = priorLtrs(modalEffectLtrs) #maxAlt=max(priorLtrs(dDomain))

plot(dDomain,priorLtrs(dDomain),typ='n',xlab="Reading-direction bias (Cohen's d) for letters",ylab="Density",ylim=c(0,max(1,1.4*maxAlt)),main=)
#typ 'n' means invisible, and it's the lines command that actually plots the points.
arrows(0,0,0,1,col='darkblue',lwd=2)
lines(dDomain,priorLtrs(dDomain),col='green',lwd=2)
legend("topright",legend=c("Null","Alternative","Previous findings"),col=c('darkblue','green','black'),lwd=2)
points(cohensDs, rep(.1,length(cohensDs)),pch=19) #plot previous vertical effect sizes
text(cohensDs, c(.2,.27), cohensDslabels,cex=.75)

points(cohensDsHoriz, rep(.1,length(cohensDsHoriz)),pch=21,col="grey",bg="grey") #plot previous horizontal arrangement effect sizes
text(cohensDsHoriz, c(.02,.06), cohensDsHorizLabels,cex=.75, col="grey")

```

```{r}
sigmaLeftWords = sigmaLeft
sigmaRightWords = 0.9
```
In contrast to the letters, prior for the words will be  a DoubleGaussian distribution with our mean observed effect (as a Cohen's d), `r wordPredictedCohensD` and sigma left of mode=`r sigmaLeft` and sigma right of mode=`r sigmaRight`.

```{r wordsPrior}
#Specify hypothesis, technically alternative hypothesis (up to constant of proportionality) for words

#Set up mhy prior, a double Gaussian, making it a function of just one parameter, the x value
myDoubleGaussian = function(x) {
  doubleGaussian(x,wordPredictedCohensD,sigmaLeftWords,sigmaRightWords)
}

#Specify hypothesis, technically alternative hypothesis (up to constant of proportionality) for letters
lo=0 #lower bound of support
hi=Inf #upper bound of support

#Define integrable prior - must be function of only the integrand
altHypothesis=function(delta) {
  y= myDoubleGaussian(delta)
  y=y*as.integer(delta>lo)*as.integer(delta<hi) #multiply by zero if outside of support
}

#Normalize alternative density in case is not already
#Calculate normalization factor - fraction to multiply prior by so it will integrate to 1
K=1/integrate(altHypothesis,lower=lo,upper=hi)$value 

#Thus final prior is the normalized function
priorWords=function(delta) K*altHypothesis(delta)
```

Plot the words prior

```{r, echo=FALSE}

#delta=seq(-.2,1.2,.01)

dDomain=seq(-.3,2.5,.01)

#Plot Alternative as a density and Null as a point arrow
maxAlt=max(priorWords(dDomain))
plot(dDomain,priorWords(dDomain),typ='n',xlab="Top bias (Cohen's d)",ylab="Density",ylim=c(0,1),main="Words and letters top bias prior")
arrows(0,0,0,1,col='darkblue',lwd=2)
lines(dDomain,priorWords(dDomain),col='yellow',lwd=3)
lines(dDomain,priorLtrs(dDomain),col='green',lwd=2)

legend("topright",legend=c("Null","AlternativeLtrs","AlternativeWords","Previous findings"),col=c('darkblue','green','yellow','black'),lwd=2)

points(cohensDs, rep(.1,length(cohensDs)),pch=19) #plot previous vertical effect sizes
text(cohensDs, c(.17,.17), cohensDslabels,cex=.75)

points(cohensDsHoriz, rep(.1,length(cohensDsHoriz)), pch=21,col="grey",bg="grey") #plot previous horizontal arrangement effect sizes
text(cohensDsHoriz, c(.02,.06), cohensDsHorizLabels, cex=.75, col="grey")
```

When the data come in, will calculate the Cohen's d and then for each Cohen's d in the prior, calculate the probability of the data, integrating over the entire prior. That average likelihood (probability of the data under the alternative hypothesis) will be compared to probability of the data under the null hypothesis.

That is, will take the likelihood of the left bias difference observed under the prior and divide it by the likelihood of the left bias difference observed under the null.

http://jeffrouder.blogspot.com.au/2016/01/what-priors-should-i-use-part-i.html
For the null model, the bias is just zero (delta function).For our model, support is zero to infinity. 

Probably best to do it in the standardized units (Cohen's d) I've been using because otherwise need to create a model of the variance separately (for the data model) so that can calculate the probability of the data given e.g. the null hypothesis.

Modelling everything as if it is a one-sample design, which is what Rouder's blog post was written for.  Because this isn't a one-sample experiment of course, instead there's an efficacy for the top position and of the bottom position. Yes because I turned it into a difference score.  And it's not like we're doing this only to simplify  for a custom Bayesian, Kim actually did that with JASP for other reasons.

"If we take a sample of n observations from a normal distribution, then the t-distribution with n-1 degrees of freedom can be defined as the distribution of the location of the sample mean relative to the true mean, divided by the sample standard deviation, after multiplying by the standardizing term sqrt(n)." - Wikipedia.

## Probability of any particular effect size according to these priors

The predicted density of data for the null is related to the central t distribution. 

So assuming that the top bias is normally distributed between participants, we can use the t.
n refers to the number of participants, and the sample standard deviation is the standard deviation of the participant effects.

In R,
dt(x, df, ncp, log = FALSE) Density for the t distribution with df degrees of freedom (and optional non-centrality parameter ncp).

```{r}
nullPredF=function(obs,N) {
  dt(sqrt(N)*obs,N-1)
}
```

Compute this predicted density for any observed effect size or for all of them. The following code does it for a reasonable range of effect sizes for a sample size of 
```{r}
obs<-seq(-.2,2.1,.01)
numSs<- 40
nullPred<- nullPredF(obs,numSs)
```
`r numSs`

Getting the predictive density for the alternative is a bit harder (Rouder). For each nonzero effect size parameter, the distribution of the observed effect follows a noncentral t distribution. Hence, to obtain predictions across all nonzero effect size parameters, we need to integrate the alternative model against the noncentral t distribution. 

```{r, echo=FALSE}
altPredIntegrandWords = function(delta,obs,N) {
  dt( sqrt(N)*obs, N-1, sqrt(N)*delta ) * priorWords(delta)
}
altPredIntegrandLtrs = function(delta,obs,N) {
  dt( sqrt(N)*obs, N-1, sqrt(N)*delta ) * priorLtrs(delta)
}

#For a particular effect size, obs, and a particular number of participants, calcuate the likelihood of it
altPredWordsF = function(obs,N) {
  integrate( altPredIntegrandWords, lower=lo, upper=hi, obs=obs, N=N )$value
}
altPredLtrsF = function(obs,N) {
  integrate( altPredIntegrandLtrs, lower=lo, upper=hi, obs=obs, N=N )$value
}
```

Now calculate the likelihood of every effect size. 

Seemingly every time it's called, it yields this warning, so I've suppressed warnings for now.

> Warning in dt(sqrt(N) * obs, N - 1, sqrt(N) * delta): full precision may not have been achieved in 'pnt{final}'

```{r, echo=FALSE, warning=FALSE}
I=length(obs)
altPredWords<- 1:I #A vector to put the likelihoods
for (i in 1:I) {
  altPredWords[i]=altPredWordsF(obs[i],numSs)
}
altPredLtrs<- 1:I #A vector to put the likelihoods
for (i in 1:I) {
  altPredLtrs[i]=altPredLtrsF(obs[i],numSs)
}
```

Now we can plot the predictions for all observed effect sizes

```{r}

top=max(altPredWords,altPredLtrs,nullPred)
tit<- paste('Predictions (n=',numSs,')')
plot(type='l',obs,nullPred,ylim=c(0,top),xlab="Observed Effect Size",ylab="Density",main=tit,col='darkblue',lwd=2)
lines(obs,altPredWords,col='yellow',lwd=2)
lines(obs,altPredLtrs,col='green',lwd=2)

legend("topright",legend=c("Null","AlternativeWords","AlternativeLtrs"),col=c('blue','yellow2','darkgreen'),lwd=2)
points(cohensDs, rep(.1,length(cohensDs)),pch=19) #plot previous vertical effect sizes
text(cohensDs, c(.17,.17), cohensDslabels,cex=.75)

points(cohensDsHoriz, rep(.1,length(cohensDsHoriz)), pch=21,col="grey",bg="grey") #plot previous horizontal arrangement effect sizes
text(cohensDsHoriz, c(.02,.05), cohensDsHorizLabels, cex=.75, col="grey")
```

I can't explain why the notch occurs in the alternative hypothesis, but it's small enough to be trivial. (actually it seems gone now)

## Sample size

 We will sample at least 40 people (after excluded participants have been omitted).  Once we have reached 40 people, we will analyse the data periodically to determine whether our results have met the Bayesian stopping rule specified below.

### Sample size rationale

See above for illustration that 40 people seem to be sufficient for the expected effect sizes.

Reaching a Bayes factor of 10 (or .1) would indicate that (a) the experimental hypothesis predicts the data are 10 times more likely than does the null hypothesis, or (b) the null hypothesis predicts the data are 10 times more likely to predict the observed data than the experimental hypothesis.  Either would be considered strong evidence.  We have added the time constraint (see below) because we will be recruiting from the first year subject pool, which closes at the end of semester.  

### Stopping rule

We will stop when:

Our analysis indicates that we have reached a Bayes factor of at least 10 (in favour of either the experimental or null hypothesis) for hypotheses 1 and 2; OR 

Our analysis indicates that we have reached a Bayes factor of at least 10 (in favour of either the experimental or null hypothesis) for hypothesis 1 and have tested at least 65 participants; OR

We reach the end of March, 2020.

THE BELOW IS JUST QUALITATIVE PLANS FOR THE OTHER MEASURES BESIDES EFFICACY (THE ABOVE QUANTITATIVE DETAILS ARE ONLY FOR THE EFFICACY ANALYSIS)

### Data exclusion

We will remove the data for the relevant condition from any participant who achieves efficacy of above 90 per cent or less than 10 per cent in both left and right streams for a given condition.

We will use an eye-tracker to ensure that participants maintain fixation throughout the trial.  We will omit any trials where the eye-tracker indicates an eye-movement of more than one degree from fixation was made. 

## Latency and precision predictions

Broad prediction is that the latency for top and bottom will be similar for letters and words and close to zero. Historically we have seen a small non-significantly higher latency in the horizontal configuration for the right visual field, for example 10 ms in HolNguGood downward condition (13 vs 23 ms), while that particular paper didn't have a latency difference for horizontal conditions. Thus a 10 ms difference is compatible with the letters condition. And not big enough to be expected if attentional shift was going on. THEREFORE DO A SLAB NULL PRIOR THAT INCLUDES 10 MS?

## Predicted: no correlation between the two streams' temporal errors.

We consistently in previously published studies observed zero correlation in the temporal errors of the two stimuli (always letters, previously) presented, except when we placed the letters very close to each other (unpublished honours thesis by Xiaoqi "Cheryl" Xu), so we predict similar for the two letters and the two words.

## Predicted: no effect of report order

We have consistently observed no significant effect of report order, but that was with typed responses and not words.

## Prediction: No diff between upper and lower words for position of errors.

Analysis of the spatial position of errors on the top and bottom. Our interest was whether the average letter position of errors was closer to the end of the word for responses in the bottom stream than the top stream. This could indicate that there was a letter-by-letter scanning process rather than the processing occurring at the whole-word level, because with a letter-by-letter account, there might not be time to get to the end of the lower word. So our prediction is that the data will favor the null hypothesis 

We will calculate the average letter position of errors for each trial (for example, trap rather than tram is a position 4 error) and averaged these for each participant.  We did this only for those trials where participants had made responses that were orthographically similar to the target (as measured by a score of 0.7 [MAYBE WE SHOULD SET THIS HIGHER?] on the van Orden (1987) orthographic similarity index, which generally accepts answers where participants get two or more of the letters correct). We then used a one-tailed Bayesian t-test to test whether the average letter position of errors was higher for responses in the right stream than the left stream. 

## Secondary research questions (not part of the main theory)

3. Whether the top bias depends on whether the streams are presented in the left or right hemifield. 
To the extent that there is a top bias, smaller capacity limit in one hemisphere, as suggested by so-and-so, could create a greater top bias.

The difference in efficacy between upper and lower streams for both letters and words depends on whether the streams are presented in the left or right visual field. STATISTICAL TEST: Compare top bias in LVF to top bias in RVF. Bayesian t-test should do it because I'm expecting a quite small effect size so the default psychology prior should be fine.

4. Previous research that has investigated lateral biases in word recognition has tended to find better performance for words presented to the right visual field, though studies that have measured this using nonwords or letter strings have found mixed results.  Overall performance may be better in the right visual field, especially for the words.

STATISTICAL TEST: ANOVA with Average efficacy  (of top and bottom) as d.v. and two factors: visual field and words versus letters.

